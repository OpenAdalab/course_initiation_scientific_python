{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Day 2 - Morning Session Exercises\n",
    "## Advanced NumPy, Pandas, and Visualization\n",
    "\n",
    "**Instructions:**\n",
    "- Complete exercises appropriate to your skill level\n",
    "- Experiment and modify the code\n",
    "- Ask questions if you get stuck!\n",
    "- Solutions are hidden below each exercise - try to solve them first!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Advanced NumPy Operations (40 min)\n",
    "\n",
    "### Physics Context\n",
    "In particle physics, we often need to calculate relationships between particles in an event - distances, angular separations, and invariant masses. Doing this efficiently requires vectorized operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### Beginner Version: Angular Separations and 2D Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated particle data for one event\n",
    "np.random.seed(42)\n",
    "n_particles = 10\n",
    "\n",
    "# Particle properties\n",
    "pt = np.random.exponential(scale=30, size=n_particles)   # Transverse momentum (GeV/c)\n",
    "eta = np.random.uniform(-2.5, 2.5, size=n_particles)     # Pseudorapidity\n",
    "phi = np.random.uniform(-np.pi, np.pi, size=n_particles) # Azimuthal angle\n",
    "\n",
    "print(f\"Generated {n_particles} particles:\")\n",
    "for i in range(n_particles):\n",
    "    print(f\"  Particle {i}: pT={pt[i]:.1f} GeV/c, Î·={eta[i]:.2f}, Ï†={phi[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate angular separation Î”R between ALL pairs of particles\n",
    "# Î”R = sqrt(Î”Î·Â² + Î”Ï†Â²)\n",
    "# Remember: Ï† wraps around (-Ï€ to Ï€), so Î”Ï† needs special handling\n",
    "\n",
    "def delta_phi(phi1, phi2):\n",
    "    \"\"\"\n",
    "    Calculate Î”Ï† accounting for wrap-around.\n",
    "    Result is in range [-Ï€, Ï€]\n",
    "    \"\"\"\n",
    "    dphi = phi1 - phi2\n",
    "    # YOUR CODE HERE: handle wrap-around\n",
    "    # Hint: use np.where to adjust values outside [-Ï€, Ï€]\n",
    "    \n",
    "    return dphi\n",
    "\n",
    "def delta_r(eta1, eta2, phi1, phi2):\n",
    "    \"\"\"\n",
    "    Calculate Î”R = sqrt(Î”Î·Â² + Î”Ï†Â²)\n",
    "    \"\"\"\n",
    "    deta = eta1 - eta2\n",
    "    dphi = delta_phi(phi1, phi2)\n",
    "    # YOUR CODE HERE: return sqrt(detaÂ² + dphiÂ²)\n",
    "    return None\n",
    "\n",
    "# Calculate Î”R for all pairs using loops (for comparison)\n",
    "n = len(eta)\n",
    "delta_r_matrix = np.zeros((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        delta_r_matrix[i, j] = delta_r(eta[i], eta[j], phi[i], phi[j])\n",
    "\n",
    "print(\"Î”R matrix (first 5x5):\")\n",
    "print(delta_r_matrix[:5, :5].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "def delta_phi(phi1, phi2):\n",
    "    \"\"\"\n",
    "    Calculate Î”Ï† accounting for wrap-around.\n",
    "    Result is in range [-Ï€, Ï€]\n",
    "    \"\"\"\n",
    "    dphi = phi1 - phi2\n",
    "    # Handle wrap-around\n",
    "    dphi = np.where(dphi > np.pi, dphi - 2*np.pi, dphi)\n",
    "    dphi = np.where(dphi < -np.pi, dphi + 2*np.pi, dphi)\n",
    "    return dphi\n",
    "\n",
    "def delta_r(eta1, eta2, phi1, phi2):\n",
    "    \"\"\"\n",
    "    Calculate Î”R = sqrt(Î”Î·Â² + Î”Ï†Â²)\n",
    "    \"\"\"\n",
    "    deta = eta1 - eta2\n",
    "    dphi = delta_phi(phi1, phi2)\n",
    "    return np.sqrt(deta**2 + dphi**2)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the closest pair of particles\n",
    "# Hint: Set diagonal to infinity so we don't find self-pairs\n",
    "\n",
    "# Make a copy to avoid modifying original\n",
    "dr_matrix = delta_r_matrix.copy()\n",
    "np.fill_diagonal(dr_matrix, np.inf)  # Exclude self-pairs\n",
    "\n",
    "# YOUR CODE HERE: Find minimum Î”R value\n",
    "min_dr = None  # Use np.min(...)\n",
    "\n",
    "# YOUR CODE HERE: Find indices of minimum\n",
    "# Hint: use np.argmin(...) then np.unravel_index to convert flat index to 2D\n",
    "i, j = 0, 0  # Replace with correct code\n",
    "\n",
    "print(f\"\\nClosest pair: particles {i} and {j}\")\n",
    "print(f\"  Î”R = {delta_r_matrix[i, j]:.3f}\")\n",
    "print(f\"  Particle {i}: Î·={eta[i]:.2f}, Ï†={phi[i]:.2f}\")\n",
    "print(f\"  Particle {j}: Î·={eta[j]:.2f}, Ï†={phi[j]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "# Make a copy to avoid modifying original\n",
    "dr_matrix = delta_r_matrix.copy()\n",
    "np.fill_diagonal(dr_matrix, np.inf)  # Exclude self-pairs\n",
    "\n",
    "# Find minimum\n",
    "min_dr = np.min(dr_matrix)\n",
    "\n",
    "# np.unravel_index converts flat index to 2D index\n",
    "i, j = np.unravel_index(np.argmin(dr_matrix), dr_matrix.shape)\n",
    "\n",
    "print(f\"\\nClosest pair: particles {i} and {j}\")\n",
    "print(f\"  Î”R = {delta_r_matrix[i, j]:.3f}\")\n",
    "print(f\"  Particle {i}: Î·={eta[i]:.2f}, Ï†={phi[i]:.2f}\")\n",
    "print(f\"  Particle {j}: Î·={eta[j]:.2f}, Ï†={phi[j]:.2f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a 2D histogram of Î·-Ï† distribution\n",
    "# Generate more particles for a better visualization\n",
    "\n",
    "np.random.seed(123)\n",
    "n_events = 10000\n",
    "\n",
    "# Simulate particles across many events\n",
    "eta_all = np.random.uniform(-2.5, 2.5, n_events)\n",
    "phi_all = np.random.uniform(-np.pi, np.pi, n_events)\n",
    "\n",
    "# Create 2D histogram\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# YOUR CODE HERE: Create 2D histogram using plt.hist2d or ax.hist2d\n",
    "# Use bins=30 for both dimensions\n",
    "# Add colorbar with label 'Events'\n",
    "\n",
    "ax.set_xlabel('Î· (pseudorapidity)')\n",
    "ax.set_ylabel('Ï† (azimuthal angle)')\n",
    "ax.set_title('Particle Distribution in Î·-Ï† Space')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "# Create 2D histogram\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create 2D histogram\n",
    "h = ax.hist2d(eta_all, phi_all, bins=30, cmap='viridis')\n",
    "plt.colorbar(h[3], ax=ax, label='Events')\n",
    "\n",
    "ax.set_xlabel('Î· (pseudorapidity)')\n",
    "ax.set_ylabel('Ï† (azimuthal angle)')\n",
    "ax.set_title('Particle Distribution in Î·-Ï† Space')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Advanced Version: Vectorized Jet Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jet clustering: Group nearby particles into jets\n",
    "# We'll implement a simple cone algorithm (not anti-kT, but educational)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_particles = 50\n",
    "\n",
    "# Generate particles (some clustered, some isolated)\n",
    "# Create 3 \"seed\" jets and spread particles around them\n",
    "jet_centers = [\n",
    "    {'eta': 0.5, 'phi': 0.3},\n",
    "    {'eta': -1.2, 'phi': -2.0},\n",
    "    {'eta': 1.8, 'phi': 1.5}\n",
    "]\n",
    "\n",
    "eta_particles = []\n",
    "phi_particles = []\n",
    "pt_particles = []\n",
    "\n",
    "for center in jet_centers:\n",
    "    n_in_jet = 15\n",
    "    eta_particles.extend(np.random.normal(center['eta'], 0.2, n_in_jet))\n",
    "    phi_particles.extend(np.random.normal(center['phi'], 0.2, n_in_jet))\n",
    "    pt_particles.extend(np.random.exponential(20, n_in_jet))\n",
    "\n",
    "# Add some random particles\n",
    "n_random = 5\n",
    "eta_particles.extend(np.random.uniform(-2.5, 2.5, n_random))\n",
    "phi_particles.extend(np.random.uniform(-np.pi, np.pi, n_random))\n",
    "pt_particles.extend(np.random.exponential(10, n_random))\n",
    "\n",
    "eta = np.array(eta_particles)\n",
    "phi = np.array(phi_particles)\n",
    "pt = np.array(pt_particles)\n",
    "\n",
    "print(f\"Generated {len(eta)} particles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement vectorized Î”R calculation for ALL pairs\n",
    "# Use broadcasting instead of loops!\n",
    "\n",
    "def compute_all_delta_r_vectorized(eta, phi):\n",
    "    \"\"\"\n",
    "    Compute Î”R between all particle pairs using broadcasting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    eta, phi : np.ndarray\n",
    "        Arrays of particle coordinates\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray : Matrix of Î”R values (shape: n x n)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Broadcasting hint: eta[:, None] has shape (n, 1), eta[None, :] has shape (1, n)\n",
    "    # Result of subtraction has shape (n, n)\n",
    "    \n",
    "    deta = None  # Calculate using broadcasting\n",
    "    dphi = None  # Calculate using broadcasting, handle wrap-around\n",
    "    dr = None    # sqrt(detaÂ² + dphiÂ²)\n",
    "    \n",
    "    return dr\n",
    "\n",
    "# Test it\n",
    "dr_matrix = compute_all_delta_r_vectorized(eta, phi)\n",
    "print(f\"Î”R matrix shape: {dr_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "def compute_all_delta_r_vectorized(eta, phi):\n",
    "    \"\"\"\n",
    "    Compute Î”R between all particle pairs using broadcasting.\n",
    "    \"\"\"\n",
    "    # Broadcasting: eta[:, None] has shape (n, 1), eta[None, :] has shape (1, n)\n",
    "    # Result of subtraction has shape (n, n)\n",
    "    \n",
    "    deta = eta[:, None] - eta[None, :]  # Shape (n, n)\n",
    "    \n",
    "    # Calculate dphi with wrap-around\n",
    "    dphi = phi[:, None] - phi[None, :]\n",
    "    dphi = np.where(dphi > np.pi, dphi - 2*np.pi, dphi)\n",
    "    dphi = np.where(dphi < -np.pi, dphi + 2*np.pi, dphi)\n",
    "    \n",
    "    dr = np.sqrt(deta**2 + dphi**2)\n",
    "    return dr\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement simple cone jet clustering\n",
    "# Algorithm:\n",
    "# 1. Start with highest pT particle as seed\n",
    "# 2. Find all particles within Î”R < R_cone\n",
    "# 3. Mark them as used, compute jet 4-momentum\n",
    "# 4. Repeat with remaining particles\n",
    "\n",
    "def cone_clustering(pt, eta, phi, R_cone=0.4, pt_min=5.0):\n",
    "    \"\"\"\n",
    "    Simple cone jet clustering algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pt, eta, phi : np.ndarray\n",
    "        Particle properties\n",
    "    R_cone : float\n",
    "        Cone radius for clustering\n",
    "    pt_min : float\n",
    "        Minimum pT for jet seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list of dict : List of jets with properties\n",
    "    \"\"\"\n",
    "    n = len(pt)\n",
    "    used = np.zeros(n, dtype=bool)\n",
    "    jets = []\n",
    "    \n",
    "    # Compute all Î”R values once\n",
    "    dr_matrix = compute_all_delta_r_vectorized(eta, phi)\n",
    "    \n",
    "    while True:\n",
    "        # Find highest pT unused particle\n",
    "        pt_masked = np.where(used, 0, pt)\n",
    "        seed_idx = np.argmax(pt_masked)\n",
    "        \n",
    "        if pt_masked[seed_idx] < pt_min:\n",
    "            break  # No more seeds above threshold\n",
    "        \n",
    "        # YOUR CODE HERE: Find particles within R_cone of seed\n",
    "        # Use dr_matrix[seed_idx] to get distances from seed\n",
    "        in_cone = None  # Boolean mask for particles in cone\n",
    "        \n",
    "        # Mark particles as used\n",
    "        used[in_cone] = True\n",
    "        used[seed_idx] = True\n",
    "        \n",
    "        # YOUR CODE HERE: Calculate jet properties (pT-weighted average)\n",
    "        jet_pt = None   # Sum of pT\n",
    "        jet_eta = None  # pT-weighted average eta\n",
    "        jet_phi = None  # pT-weighted average phi\n",
    "        \n",
    "        jets.append({\n",
    "            'pt': jet_pt,\n",
    "            'eta': jet_eta,\n",
    "            'phi': jet_phi,\n",
    "            'n_constituents': np.sum(in_cone) + 1\n",
    "        })\n",
    "    \n",
    "    return jets\n",
    "\n",
    "# Run clustering\n",
    "jets = cone_clustering(pt, eta, phi, R_cone=0.4)\n",
    "print(f\"\\nFound {len(jets)} jets:\")\n",
    "for i, jet in enumerate(jets):\n",
    "    print(f\"  Jet {i}: pT={jet['pt']:.1f}, Î·={jet['eta']:.2f}, Ï†={jet['phi']:.2f}, n={jet['n_constituents']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "def cone_clustering(pt, eta, phi, R_cone=0.4, pt_min=5.0):\n",
    "    \"\"\"\n",
    "    Simple cone jet clustering algorithm.\n",
    "    \"\"\"\n",
    "    n = len(pt)\n",
    "    used = np.zeros(n, dtype=bool)\n",
    "    jets = []\n",
    "    \n",
    "    # Compute all Î”R values once\n",
    "    dr_matrix = compute_all_delta_r_vectorized(eta, phi)\n",
    "    \n",
    "    while True:\n",
    "        # Find highest pT unused particle\n",
    "        pt_masked = np.where(used, 0, pt)\n",
    "        seed_idx = np.argmax(pt_masked)\n",
    "        \n",
    "        if pt_masked[seed_idx] < pt_min:\n",
    "            break  # No more seeds above threshold\n",
    "        \n",
    "        # Find particles within R_cone of seed\n",
    "        in_cone = (dr_matrix[seed_idx] < R_cone) & ~used\n",
    "        \n",
    "        # Mark particles as used\n",
    "        used[in_cone] = True\n",
    "        used[seed_idx] = True\n",
    "        \n",
    "        # Calculate jet properties (pT-weighted average)\n",
    "        jet_pt = np.sum(pt[in_cone]) + pt[seed_idx]\n",
    "        \n",
    "        # pT-weighted averages for eta and phi\n",
    "        weights = np.concatenate([[pt[seed_idx]], pt[in_cone]])\n",
    "        eta_vals = np.concatenate([[eta[seed_idx]], eta[in_cone]])\n",
    "        phi_vals = np.concatenate([[phi[seed_idx]], phi[in_cone]])\n",
    "        \n",
    "        jet_eta = np.average(eta_vals, weights=weights)\n",
    "        jet_phi = np.average(phi_vals, weights=weights)\n",
    "        \n",
    "        jets.append({\n",
    "            'pt': jet_pt,\n",
    "            'eta': jet_eta,\n",
    "            'phi': jet_phi,\n",
    "            'n_constituents': np.sum(in_cone) + 1\n",
    "        })\n",
    "    \n",
    "    return jets\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the jets\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot all particles\n",
    "scatter = ax.scatter(eta, phi, c=pt, s=pt*2, cmap='viridis', alpha=0.6, label='Particles')\n",
    "plt.colorbar(scatter, ax=ax, label='pT (GeV/c)')\n",
    "\n",
    "# Plot jet cones\n",
    "for i, jet in enumerate(jets):\n",
    "    circle = plt.Circle((jet['eta'], jet['phi']), 0.4, \n",
    "                        fill=False, color='red', linewidth=2, linestyle='--')\n",
    "    ax.add_patch(circle)\n",
    "    ax.plot(jet['eta'], jet['phi'], 'r*', markersize=15)\n",
    "    ax.annotate(f\"Jet {i}\\npT={jet['pt']:.0f}\", \n",
    "                (jet['eta'], jet['phi']), \n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                fontsize=9, color='red')\n",
    "\n",
    "ax.set_xlabel('Î·')\n",
    "ax.set_ylabel('Ï†')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-np.pi - 0.5, np.pi + 0.5)\n",
    "ax.set_title('Jet Clustering Visualization')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Benchmark vectorized vs loop approach\n",
    "\n",
    "def delta_r_loops(eta, phi):\n",
    "    \"\"\"Loop-based Î”R calculation.\"\"\"\n",
    "    n = len(eta)\n",
    "    result = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            deta = eta[i] - eta[j]\n",
    "            dphi = phi[i] - phi[j]\n",
    "            if dphi > np.pi:\n",
    "                dphi -= 2*np.pi\n",
    "            if dphi < -np.pi:\n",
    "                dphi += 2*np.pi\n",
    "            result[i, j] = np.sqrt(deta**2 + dphi**2)\n",
    "    return result\n",
    "\n",
    "# Test with different sizes\n",
    "sizes = [50, 100, 200, 500]\n",
    "\n",
    "print(\"Performance comparison:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for size in sizes:\n",
    "    eta_test = np.random.uniform(-2.5, 2.5, size)\n",
    "    phi_test = np.random.uniform(-np.pi, np.pi, size)\n",
    "    \n",
    "    # Time loops\n",
    "    start = time.time()\n",
    "    result_loops = delta_r_loops(eta_test, phi_test)\n",
    "    time_loops = time.time() - start\n",
    "    \n",
    "    # Time vectorized\n",
    "    start = time.time()\n",
    "    result_vec = compute_all_delta_r_vectorized(eta_test, phi_test)\n",
    "    time_vec = time.time() - start\n",
    "    \n",
    "    speedup = time_loops / time_vec if time_vec > 0 else float('inf')\n",
    "    print(f\"n={size:4d}: Loops={time_loops*1000:8.2f}ms, Vec={time_vec*1000:6.2f}ms, Speedup={speedup:6.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2.2: Advanced Pandas Techniques (45 min)\n",
    "\n",
    "### Physics Context\n",
    "Real particle physics data has hierarchical structure: runs contain events, events contain particles. We need efficient ways to work with this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Beginner Version: Energy Calibrations and Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated detector data\n",
    "np.random.seed(42)\n",
    "n_events = 1000\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'run': np.random.choice([1, 2, 3, 4], n_events),\n",
    "    'event': range(n_events),\n",
    "    'detector': np.random.choice(['barrel', 'endcap'], n_events),\n",
    "    'energy_raw': np.random.exponential(50, n_events),\n",
    "    'eta': np.random.uniform(-2.5, 2.5, n_events)\n",
    "})\n",
    "\n",
    "print(\"Raw data:\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply energy calibrations\n",
    "# Different calibration factors for each detector region and run\n",
    "\n",
    "calibration = {\n",
    "    (1, 'barrel'): 1.02,\n",
    "    (1, 'endcap'): 1.05,\n",
    "    (2, 'barrel'): 1.01,\n",
    "    (2, 'endcap'): 1.04,\n",
    "    (3, 'barrel'): 1.03,\n",
    "    (3, 'endcap'): 1.06,\n",
    "    (4, 'barrel'): 1.00,\n",
    "    (4, 'endcap'): 1.03,\n",
    "}\n",
    "\n",
    "# Method 1: Using apply (slow but simple)\n",
    "def get_calibration(row):\n",
    "    key = (row['run'], row['detector'])\n",
    "    return row['energy_raw'] * calibration.get(key, 1.0)\n",
    "\n",
    "# YOUR CODE HERE: use apply method to calibrate energy\n",
    "data['energy_cal_v1'] = None\n",
    "\n",
    "# Method 2: More efficient - create a calibration DataFrame and merge\n",
    "cal_df = pd.DataFrame([\n",
    "    {'run': k[0], 'detector': k[1], 'cal_factor': v}\n",
    "    for k, v in calibration.items()\n",
    "])\n",
    "\n",
    "# YOUR CODE HERE: Merge calibration factors and calculate calibrated energy\n",
    "# Step 1: Merge data with cal_df on ['run', 'detector']\n",
    "# Step 2: Calculate energy_cal_v2 = energy_raw * cal_factor\n",
    "\n",
    "print(\"\\nWith calibration:\")\n",
    "data[['run', 'detector', 'energy_raw', 'energy_cal_v1']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "# Method 1: Using apply (slow but simple)\n",
    "data['energy_cal_v1'] = data.apply(get_calibration, axis=1)\n",
    "\n",
    "# Method 2: More efficient - create a calibration DataFrame and merge\n",
    "cal_df = pd.DataFrame([\n",
    "    {'run': k[0], 'detector': k[1], 'cal_factor': v}\n",
    "    for k, v in calibration.items()\n",
    "])\n",
    "\n",
    "# Merge calibration factors\n",
    "data = pd.merge(data, cal_df, on=['run', 'detector'], how='left')\n",
    "\n",
    "# Calculate calibrated energy\n",
    "data['energy_cal_v2'] = data['energy_raw'] * data['cal_factor']\n",
    "\n",
    "print(\"\\nWith calibration:\")\n",
    "data[['run', 'detector', 'energy_raw', 'cal_factor', 'energy_cal_v1', 'energy_cal_v2']].head(10)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create pivot tables for run-by-run statistics\n",
    "\n",
    "# Pivot table: mean energy by run and detector\n",
    "pivot_mean = pd.pivot_table(\n",
    "    data,\n",
    "    values='energy_cal_v1',\n",
    "    index='run',\n",
    "    columns='detector',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print(\"Mean calibrated energy by run and detector:\")\n",
    "print(pivot_mean.round(2))\n",
    "\n",
    "# YOUR CODE HERE: Create pivot table with multiple statistics\n",
    "# Use aggfunc=['mean', 'std', 'count']\n",
    "pivot_multi = None\n",
    "\n",
    "print(\"\\nMultiple statistics:\")\n",
    "# print(pivot_multi.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "# Create pivot table with multiple statistics\n",
    "pivot_multi = pd.pivot_table(\n",
    "    data,\n",
    "    values='energy_cal_v1',\n",
    "    index='run',\n",
    "    columns='detector',\n",
    "    aggfunc=['mean', 'std', 'count']\n",
    ")\n",
    "\n",
    "print(\"\\nMultiple statistics:\")\n",
    "print(pivot_multi.round(2))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize run-by-run differences\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Energy distribution by run\n",
    "for run in sorted(data['run'].unique()):\n",
    "    subset = data[data['run'] == run]['energy_cal_v1']\n",
    "    axes[0].hist(subset, bins=30, alpha=0.5, label=f'Run {run}', range=(0, 200))\n",
    "\n",
    "axes[0].set_xlabel('Calibrated Energy (GeV)')\n",
    "axes[0].set_ylabel('Events')\n",
    "axes[0].set_title('Energy Distribution by Run')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Mean energy vs run (bar chart)\n",
    "mean_by_run = data.groupby(['run', 'detector'])['energy_cal_v1'].mean().unstack()\n",
    "mean_by_run.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_xlabel('Run')\n",
    "axes[1].set_ylabel('Mean Energy (GeV)')\n",
    "axes[1].set_title('Mean Energy by Run and Detector')\n",
    "axes[1].legend(title='Detector')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Advanced Version: Hierarchical Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hierarchical dataset: Run â†’ Event â†’ Particle\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data\n",
    "data_list = []\n",
    "\n",
    "for run in range(1, 4):  # 3 runs\n",
    "    n_events_in_run = np.random.randint(30, 50)\n",
    "    for event in range(n_events_in_run):\n",
    "        n_particles = np.random.poisson(5)  # ~5 particles per event\n",
    "        for particle in range(n_particles):\n",
    "            data_list.append({\n",
    "                'run': run,\n",
    "                'event': event,\n",
    "                'particle': particle,\n",
    "                'particle_type': np.random.choice(['electron', 'muon', 'photon', 'jet']),\n",
    "                'pt': np.random.exponential(30),\n",
    "                'eta': np.random.uniform(-2.5, 2.5),\n",
    "                'phi': np.random.uniform(-np.pi, np.pi),\n",
    "                'energy': np.random.exponential(50)\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "print(f\"Total particles: {len(df)}\")\n",
    "print(f\"Runs: {df['run'].nunique()}, Events: {df.groupby('run')['event'].nunique().sum()}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create MultiIndex DataFrame\n",
    "\n",
    "# YOUR CODE HERE: Set index to ['run', 'event', 'particle']\n",
    "df_multi = None\n",
    "\n",
    "print(\"MultiIndex DataFrame:\")\n",
    "# print(df_multi.head(15))\n",
    "\n",
    "# Access data at different levels\n",
    "# print(\"\\n--- Run 1 data ---\")\n",
    "# print(df_multi.loc[1].head())\n",
    "\n",
    "# print(\"\\n--- Run 1, Event 0 ---\")\n",
    "# print(df_multi.loc[(1, 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "# Create MultiIndex DataFrame\n",
    "df_multi = df.set_index(['run', 'event', 'particle'])\n",
    "\n",
    "print(\"MultiIndex DataFrame:\")\n",
    "print(df_multi.head(15))\n",
    "\n",
    "# Access data at different levels\n",
    "print(\"\\n--- Run 1 data ---\")\n",
    "print(df_multi.loc[1].head())\n",
    "\n",
    "print(\"\\n--- Run 1, Event 0 ---\")\n",
    "print(df_multi.loc[(1, 0)])\n",
    "\n",
    "print(\"\\n--- Using xs (cross-section) ---\")\n",
    "print(df_multi.xs(1, level='run').head())\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate event-level quantities using groupby\n",
    "\n",
    "# Group by run and event\n",
    "event_grouped = df.groupby(['run', 'event'])\n",
    "\n",
    "# YOUR CODE HERE: Calculate event-level quantities\n",
    "# - Total pT (sum)\n",
    "# - Leading pT (max)\n",
    "# - Total energy (sum)\n",
    "# - Number of particles (count)\n",
    "\n",
    "event_summary = event_grouped.agg({\n",
    "    # YOUR CODE HERE\n",
    "})\n",
    "\n",
    "print(\"Event-level summary:\")\n",
    "# print(event_summary.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "# Group by run and event\n",
    "event_grouped = df.groupby(['run', 'event'])\n",
    "\n",
    "# Calculate event-level quantities\n",
    "event_summary = event_grouped.agg({\n",
    "    'pt': ['sum', 'max'],  # Total pT, leading pT\n",
    "    'energy': 'sum',        # Total energy\n",
    "    'particle': 'count'     # Number of particles\n",
    "})\n",
    "\n",
    "# Flatten column names\n",
    "event_summary.columns = ['_'.join(col).strip() for col in event_summary.columns]\n",
    "event_summary = event_summary.rename(columns={'particle_count': 'n_particles'})\n",
    "\n",
    "print(\"Event-level summary:\")\n",
    "print(event_summary.head(10))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find leading particle in each event\n",
    "\n",
    "# YOUR CODE HERE: Use idxmax to find index of max pT in each event\n",
    "# Then use .loc to get those particles\n",
    "\n",
    "leading_idx = None  # df.groupby(['run', 'event'])['pt'].idxmax()\n",
    "leading_particles = None  # df.loc[leading_idx]\n",
    "\n",
    "print(\"Leading particles (first 10 events):\")\n",
    "# print(leading_particles[['run', 'event', 'particle_type', 'pt', 'eta']].head(10))\n",
    "\n",
    "# Statistics of leading particles\n",
    "print(\"\\nLeading particle type distribution:\")\n",
    "# print(leading_particles['particle_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "# Find leading particle in each event using idxmax\n",
    "leading_idx = df.groupby(['run', 'event'])['pt'].idxmax()\n",
    "leading_particles = df.loc[leading_idx]\n",
    "\n",
    "print(\"Leading particles (first 10 events):\")\n",
    "print(leading_particles[['run', 'event', 'particle_type', 'pt', 'eta']].head(10))\n",
    "\n",
    "# Statistics of leading particles\n",
    "print(\"\\nLeading particle type distribution:\")\n",
    "print(leading_particles['particle_type'].value_counts())\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Memory optimization with categorical types\n",
    "\n",
    "print(\"Memory usage before optimization:\")\n",
    "print(df.memory_usage(deep=True))\n",
    "print(f\"Total: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "# YOUR CODE HERE: Convert 'particle_type' to categorical\n",
    "df_optimized = df.copy()\n",
    "# df_optimized['particle_type'] = ...\n",
    "\n",
    "print(\"\\nMemory usage after optimization:\")\n",
    "print(df_optimized.memory_usage(deep=True))\n",
    "print(f\"Total: {df_optimized.memory_usage(deep=True).sum() / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "print(\"Memory usage before optimization:\")\n",
    "print(df.memory_usage(deep=True))\n",
    "print(f\"Total: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "# Convert string columns to categorical\n",
    "df_optimized = df.copy()\n",
    "df_optimized['particle_type'] = df_optimized['particle_type'].astype('category')\n",
    "\n",
    "print(\"\\nMemory usage after optimization:\")\n",
    "print(df_optimized.memory_usage(deep=True))\n",
    "print(f\"Total: {df_optimized.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "reduction = 1 - df_optimized.memory_usage(deep=True).sum() / df.memory_usage(deep=True).sum()\n",
    "print(f\"\\nMemory reduction: {reduction*100:.1f}%\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2.3: Visualization with Matplotlib and Seaborn (30 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### Beginner Version: Standard Analysis Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated Z boson data\n",
    "np.random.seed(42)\n",
    "n_events = 5000\n",
    "\n",
    "# Signal: Z boson mass peak\n",
    "mass_signal = np.random.normal(91.2, 2.5, int(n_events * 0.7))\n",
    "# Background: exponential\n",
    "mass_background = np.random.exponential(30, int(n_events * 0.3)) + 60\n",
    "mass_background = mass_background[mass_background < 120]\n",
    "\n",
    "mass_all = np.concatenate([mass_signal, mass_background])\n",
    "pt_all = np.random.exponential(40, len(mass_all))\n",
    "\n",
    "print(f\"Generated {len(mass_all)} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a mass peak plot with proper formatting\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# YOUR CODE HERE: Create histogram with histtype='step'\n",
    "# counts, bins, _ = ax.hist(...)\n",
    "\n",
    "# YOUR CODE HERE: Add error bars (Poisson errors = sqrt(N))\n",
    "# bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "# errors = np.sqrt(counts)\n",
    "# ax.errorbar(...)\n",
    "\n",
    "# YOUR CODE HERE: Add proper labels\n",
    "ax.set_xlabel(r'$m_{\\mu\\mu}$ (GeV/cÂ²)', fontsize=14)\n",
    "ax.set_ylabel('Events / (1.5 GeV/cÂ²)', fontsize=14)\n",
    "ax.set_title('Dimuon Invariant Mass Distribution', fontsize=16)\n",
    "\n",
    "# Add annotation for Z peak\n",
    "# ax.annotate(...)\n",
    "\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Create histogram\n",
    "counts, bins, _ = ax.hist(mass_all, bins=40, range=(60, 120), \n",
    "                          histtype='step', linewidth=2, color='black',\n",
    "                          label='Data')\n",
    "\n",
    "# Add error bars\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "errors = np.sqrt(counts)\n",
    "ax.errorbar(bin_centers, counts, yerr=errors, fmt='none', \n",
    "            capsize=2, color='black', label='Stat. uncertainty')\n",
    "\n",
    "# Add proper labels\n",
    "ax.set_xlabel(r'$m_{\\mu\\mu}$ (GeV/cÂ²)', fontsize=14)\n",
    "ax.set_ylabel('Events / (1.5 GeV/cÂ²)', fontsize=14)\n",
    "ax.set_title('Dimuon Invariant Mass Distribution', fontsize=16)\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate('Z peak', xy=(91.2, max(counts)*0.9), fontsize=12,\n",
    "            ha='center', color='red')\n",
    "ax.axvline(91.2, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create pT distribution plot with log scale\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# YOUR CODE HERE: Create histogram of pT\n",
    "# Use bins=50, range=(0, 200)\n",
    "\n",
    "# YOUR CODE HERE: Add log scale\n",
    "# ax.set_yscale('log')\n",
    "\n",
    "ax.set_xlabel(r'$p_T$ (GeV/c)', fontsize=14)\n",
    "ax.set_ylabel('Events', fontsize=14)\n",
    "ax.set_title(r'Transverse Momentum Distribution', fontsize=16)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Create histogram of pT\n",
    "ax.hist(pt_all, bins=50, range=(0, 200), histtype='step', \n",
    "        linewidth=2, color='blue', label='All events')\n",
    "\n",
    "# Add log scale\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_xlabel(r'$p_T$ (GeV/c)', fontsize=14)\n",
    "ax.set_ylabel('Events', fontsize=14)\n",
    "ax.set_title(r'Transverse Momentum Distribution', fontsize=16)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "### Advanced Version: Multi-Panel Data/MC Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate \"data\" and \"Monte Carlo\" samples\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data\n",
    "n_data = 10000\n",
    "data_mass = np.concatenate([\n",
    "    np.random.normal(91.2, 2.5, int(n_data * 0.7)),\n",
    "    np.random.exponential(25, int(n_data * 0.3)) + 60\n",
    "])\n",
    "data_mass = data_mass[(data_mass > 60) & (data_mass < 120)]\n",
    "\n",
    "# MC Signal (normalized to data)\n",
    "n_mc_sig = int(n_data * 0.7 * 1.2)  # Slightly more for better stats\n",
    "mc_signal = np.random.normal(91.2, 2.5, n_mc_sig)\n",
    "\n",
    "# MC Background\n",
    "n_mc_bkg = int(n_data * 0.3 * 1.2)\n",
    "mc_background = np.random.exponential(25, n_mc_bkg) + 60\n",
    "\n",
    "print(f\"Data: {len(data_mass)}, MC Signal: {len(mc_signal)}, MC Background: {len(mc_background)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create publication-quality comparison plot with:\n",
    "# - Top panel: Data points with error bars + stacked MC histograms\n",
    "# - Bottom panel: Data/MC ratio\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10), \n",
    "                         gridspec_kw={'height_ratios': [3, 1]},\n",
    "                         sharex=True)\n",
    "\n",
    "# Define binning\n",
    "bins = np.linspace(60, 120, 41)\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "bin_width = bins[1] - bins[0]\n",
    "\n",
    "# ===== Top panel: Data and MC =====\n",
    "ax1 = axes[0]\n",
    "\n",
    "# YOUR CODE HERE: Create data histogram and get counts\n",
    "# data_counts, _ = np.histogram(data_mass, bins=bins)\n",
    "# data_errors = np.sqrt(data_counts)\n",
    "\n",
    "# YOUR CODE HERE: Plot data as points with error bars\n",
    "# ax1.errorbar(...)\n",
    "\n",
    "# YOUR CODE HERE: Create MC histograms and scale to data\n",
    "# mc_sig_counts, _ = np.histogram(mc_signal, bins=bins)\n",
    "# mc_bkg_counts, _ = np.histogram(mc_background, bins=bins)\n",
    "# scale = data_counts.sum() / (mc_sig_counts.sum() + mc_bkg_counts.sum())\n",
    "\n",
    "# YOUR CODE HERE: Plot stacked MC using ax1.bar()\n",
    "\n",
    "ax1.set_ylabel('Events / (1.5 GeV/cÂ²)', fontsize=12)\n",
    "ax1.set_title('Z â†’ Î¼Î¼: Data vs Monte Carlo', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ===== Bottom panel: Data/MC ratio =====\n",
    "ax2 = axes[1]\n",
    "\n",
    "# YOUR CODE HERE: Calculate and plot ratio\n",
    "# ratio = data_counts / mc_total\n",
    "# ax2.errorbar(...)\n",
    "# ax2.axhline(1.0, ...)\n",
    "\n",
    "ax2.set_xlabel(r'$m_{\\mu\\mu}$ (GeV/cÂ²)', fontsize=12)\n",
    "ax2.set_ylabel('Data / MC', fontsize=12)\n",
    "ax2.set_ylim(0.5, 1.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35-solution",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10), \n",
    "                         gridspec_kw={'height_ratios': [3, 1]},\n",
    "                         sharex=True)\n",
    "\n",
    "# Define binning\n",
    "bins = np.linspace(60, 120, 41)\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "bin_width = bins[1] - bins[0]\n",
    "\n",
    "# ===== Top panel: Data and MC =====\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Data histogram\n",
    "data_counts, _ = np.histogram(data_mass, bins=bins)\n",
    "data_errors = np.sqrt(data_counts)\n",
    "ax1.errorbar(bin_centers, data_counts, yerr=data_errors, \n",
    "             fmt='ko', markersize=4, label='Data')\n",
    "\n",
    "# MC histograms (stacked)\n",
    "mc_sig_counts, _ = np.histogram(mc_signal, bins=bins)\n",
    "mc_bkg_counts, _ = np.histogram(mc_background, bins=bins)\n",
    "\n",
    "# Scale MC to data\n",
    "scale = data_counts.sum() / (mc_sig_counts.sum() + mc_bkg_counts.sum())\n",
    "mc_sig_scaled = mc_sig_counts * scale\n",
    "mc_bkg_scaled = mc_bkg_counts * scale\n",
    "\n",
    "# Plot stacked MC\n",
    "ax1.bar(bin_centers, mc_bkg_scaled, width=bin_width, alpha=0.5, \n",
    "        color='orange', label='MC Background')\n",
    "ax1.bar(bin_centers, mc_sig_scaled, width=bin_width, alpha=0.5,\n",
    "        bottom=mc_bkg_scaled, color='blue', label='MC Signal')\n",
    "\n",
    "ax1.set_ylabel('Events / (1.5 GeV/cÂ²)', fontsize=12)\n",
    "ax1.set_title('Z â†’ Î¼Î¼: Data vs Monte Carlo', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add experiment label\n",
    "ax1.text(0.05, 0.95, 'Simulation\\n' + r'$\\sqrt{s}$ = 13 TeV',\n",
    "         transform=ax1.transAxes, verticalalignment='top',\n",
    "         fontsize=11, family='sans-serif')\n",
    "\n",
    "# ===== Bottom panel: Data/MC ratio =====\n",
    "ax2 = axes[1]\n",
    "\n",
    "mc_total = mc_sig_scaled + mc_bkg_scaled\n",
    "ratio = np.divide(data_counts, mc_total, where=mc_total > 0)\n",
    "ratio_err = np.divide(data_errors, mc_total, where=mc_total > 0)\n",
    "\n",
    "ax2.errorbar(bin_centers, ratio, yerr=ratio_err, fmt='ko', markersize=4)\n",
    "ax2.axhline(1.0, color='red', linestyle='--', linewidth=1)\n",
    "ax2.fill_between([60, 120], [0.9, 0.9], [1.1, 1.1], alpha=0.2, color='gray')\n",
    "\n",
    "ax2.set_xlabel(r'$m_{\\mu\\mu}$ (GeV/cÂ²)', fontsize=12)\n",
    "ax2.set_ylabel('Data / MC', fontsize=12)\n",
    "ax2.set_ylim(0.5, 1.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_mc_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn statistical visualization\n",
    "\n",
    "# Create DataFrame for Seaborn\n",
    "df_plot = pd.DataFrame({\n",
    "    'pt': np.concatenate([pt_all, np.random.exponential(35, len(pt_all))]),\n",
    "    'eta': np.concatenate([\n",
    "        np.random.uniform(-2.5, 2.5, len(pt_all)),\n",
    "        np.random.uniform(-2.5, 2.5, len(pt_all))\n",
    "    ]),\n",
    "    'source': ['Data'] * len(pt_all) + ['MC'] * len(pt_all)\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# KDE comparison\n",
    "sns.kdeplot(data=df_plot, x='pt', hue='source', ax=axes[0], fill=True, alpha=0.5)\n",
    "axes[0].set_xlabel(r'$p_T$ (GeV/c)')\n",
    "axes[0].set_title('pT Distribution Comparison')\n",
    "axes[0].set_xlim(0, 200)\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(data=df_plot, x='source', y='pt', ax=axes[1])\n",
    "axes[1].set_ylabel(r'$p_T$ (GeV/c)')\n",
    "axes[1].set_title('pT Box Plot')\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(data=df_plot, x='source', y='eta', ax=axes[2])\n",
    "axes[2].set_ylabel('Î·')\n",
    "axes[2].set_title('Î· Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Today you learned:\n",
    "\n",
    "âœ… **Advanced NumPy**: Vectorized operations, broadcasting, fancy indexing  \n",
    "âœ… **Performance**: Vectorized code is 10-100x faster than loops  \n",
    "âœ… **Advanced Pandas**: MultiIndex, apply/transform, memory optimization  \n",
    "âœ… **Visualization**: Publication-quality plots, Data/MC comparisons, Seaborn  \n",
    "\n",
    "**This afternoon:** Functions and Object-Oriented Programming for organizing analysis code!\n",
    "\n",
    "---\n",
    "\n",
    "**Great work! ðŸŽ‰**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
